{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems based on explicit feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "* python packages - see requirements.txt\n",
    "* dataset - you can use the movielens small, which contains 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. It was updated on 9/2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import movielens small you can use the surprise built-in dataset utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or alternatively download the movielens dataset from https://files.grouplens.org/datasets/movielens/ml-latest-small.zip and import it in surprise with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv (\"ml-latest-small/ratings.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['timestamp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "There exist multiple metrics to evaluate quality of recommendation systems:\n",
    "* accuracy metrics (root mean square error, mean absolute error), i.e., based on the difference between the predicted rating and the actual rating; RMSE is what was also used for the netflix 1 Million prize. However, it turned out that RMSE is not a good evaluation metric for recommendation systems, given that it focuses too much on rating prediction rather than on which contents are most likely to be useful for the user.\n",
    "* hit rate is the frequency of conversion of a recommended result into a click, a rating, or a purchase. For evaluation purposes, this can be computed as the ratio $\\frac{hits}{users}$.\n",
    "* average reciprocal hit rank (ARHR) is a measure similar to hit rate but it can be used for top-n recommenders to evaluate the actual rank of predicted items, given that it is computed as $\\frac{\\sum_{i=1}^{n} \\frac{1}{rank_i}}{users}$.\n",
    "* coverage gives a sense of how much of the catalog is returned by the recommendation and how quickly new content can appear in the results\n",
    "* novelty tells how popular are results in the returned recommendation, given the strong hypothesis that users are familiar with popular content and may tend to reject unknown ones. Popularity tends to be a good indication of success and therefore exploits user trust in the system, while coverage imply exploring new things user may like but may also be less known.\n",
    "* diversity tells how different are results in the returned recommendation, that is 1 - S, with S being the average similarity between all recommendation pairs in the returned recommendation;\n",
    "* churn indicates how often recommendations change, that is how quickly the results change upon user actions;\n",
    "\n",
    "While the presented metrics are good evaluation indicators, results from A/B tests will matter most as they indicate the quality of the system on actual users, rather than the test set. This is both because the test set may not be representative enough, as well as because A/B tests allow for the collection of actual sales data, which is what matter the most in the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The surprise accuracy package provides a number of evaluation metrics:\n",
    "* rmse\tCompute RMSE (Root Mean Squared Error).\n",
    "* mse\tCompute MSE (Mean Squared Error).\n",
    "* mae\tCompute MAE (Mean Absolute Error).\n",
    "* fcp\tCompute FCP (Fraction of Concordant Pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8757482354259867"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://surprise.readthedocs.io/en/stable/getting_started.html#train-test-split-and-the-fit-method\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9541696731691143"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = KNNBasic()\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking algorithm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.8786  0.8631  0.8686  0.8787  0.8754  0.8729  0.0061  \n",
      "MAE (testset)     0.6768  0.6631  0.6676  0.6740  0.6735  0.6710  0.0050  \n",
      "Fit time          4.27    4.20    4.22    4.11    4.06    4.17    0.08    \n",
      "Test time         0.27    0.11    0.11    0.11    0.12    0.14    0.07    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.87857721, 0.86310031, 0.86864053, 0.87866605, 0.87539098]),\n",
       " 'test_mae': array([0.67680962, 0.66308103, 0.66758322, 0.67396526, 0.67349298]),\n",
       " 'fit_time': (4.271094083786011,\n",
       "  4.204852104187012,\n",
       "  4.21667218208313,\n",
       "  4.109671115875244,\n",
       "  4.059509038925171),\n",
       " 'test_time': (0.2738502025604248,\n",
       "  0.10553693771362305,\n",
       "  0.10870695114135742,\n",
       "  0.10980987548828125,\n",
       "  0.11524701118469238)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://surprise.readthedocs.io/en/stable/getting_started.html#automatic-cross-validation\n",
    "from surprise import SVD\n",
    "\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "  * http://surprise.readthedocs.io/en/stable/\n",
    "  * http://media.sundog-soft.com/RecSys/RecSys.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f98fccc0b2bc22a562b743e4356d454e0e024de09e28aedc6ee7b1824b634ad"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('recsys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
